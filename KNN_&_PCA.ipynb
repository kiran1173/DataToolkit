{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA"
      ],
      "metadata": {
        "id": "_jQQOWTU0V56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        " - **K-Nearest Neighbors (KNN)** is a simple yet powerful **supervised machine learning algorithm** used for both **classification and regression** problems, and it works on the idea that **similar data points exist close to each other in the feature space**. KNN is a **lazy learning algorithm**, meaning it does not build an explicit model during training but instead stores all the training data and performs computation only at the time of prediction. When a new data point is given, KNN calculates the **distance** (usually Euclidean distance) between this point and all points in the training dataset, then selects the **K closest neighbors**. In **classification**, the new data point is assigned the class that is **most common among its K nearest neighbors** using majority voting, while in **regression**, the prediction is made by taking the **average of the target values** of the K nearest neighbors. The value of K plays a crucial role: a **small K** makes the model sensitive to noise and may cause overfitting, while a **large K** makes it more stable but may lead to underfitting. KNN is easy to understand and implement, works well for **small to medium-sized datasets**, and is widely used in applications such as **recommendation systems, pattern recognition, medical diagnosis, and image classification**.\n"
      ],
      "metadata": {
        "id": "150BRSJZ0Xwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        " - The **Curse of Dimensionality** refers to the set of problems that arise when working with **high-dimensional data**, where the number of features becomes very large. As the number of dimensions increases, the **distance between data points becomes less meaningful** because all points tend to appear almost equally far from each other. Since **K-Nearest Neighbors (KNN)** relies entirely on distance calculations to find the closest neighbors, this directly affects its performance. In high dimensions, KNN struggles to clearly distinguish between nearby and faraway points, which leads to **poor classification or regression accuracy**. Additionally, the volume of the feature space increases exponentially, making the available data appear **sparse**, so KNN needs a much larger amount of data to maintain reliable predictions. High dimensionality also increases **computational cost** and slows down prediction time because distances must be calculated for many features. As a result, the curse of dimensionality makes KNN **less efficient, less accurate, and more sensitive to noise**, and this is why **feature selection, dimensionality reduction techniques like PCA, and proper data scaling** are often applied before using KNN.\n"
      ],
      "metadata": {
        "id": "7GV5PAxu0je2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        " - **Principal Component Analysis (PCA)** is a powerful **dimensionality reduction technique** used in machine learning and data analysis to reduce the number of input features while preserving as much important information (variance) from the original dataset as possible. PCA works by transforming the original correlated features into a new set of **uncorrelated variables called principal components**, where each component is a linear combination of the original features and is ranked according to the amount of variance it captures. The first principal component captures the maximum variance, the second captures the next highest variance, and so on. By selecting only the top few principal components, we can represent the data in a lower-dimensional space with minimal loss of information, which helps in **reducing noise, improving computational efficiency, and avoiding the curse of dimensionality**. In contrast, **feature selection** does not create new features but instead **selects a subset of the original features** based on their importance, relevance, or statistical relationship with the output variable. The key difference is that **PCA transforms and combines features**, which may reduce interpretability, while **feature selection keeps the original features**, making the model easier to interpret. In simple terms, PCA creates **new compressed features**, whereas feature selection **chooses the best existing features**.\n"
      ],
      "metadata": {
        "id": "gCgqpMR00tBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        " - In **Principal Component Analysis (PCA)**, **eigenvalues and eigenvectors** are fundamental mathematical concepts that determine how the data is transformed and reduced in dimensionality. When PCA is applied, it first computes the **covariance matrix** of the dataset to understand how the features vary with respect to each other. From this covariance matrix, **eigenvectors** represent the **directions (axes)** along which the data varies the most, while the corresponding **eigenvalues** represent the **amount of variance (importance)** captured in those directions. In simple terms, an eigenvector shows the **new direction of a principal component**, and its eigenvalue tells us **how much information or variance is present along that direction**. The eigenvector with the **largest eigenvalue becomes the first principal component**, capturing the maximum variance in the data, the second largest eigenvalue gives the second principal component, and so on. These are important because PCA selects only the top eigenvectors with the highest eigenvalues to form a lower-dimensional space, ensuring that **maximum useful information is retained while reducing the number of features**. Therefore, eigenvalues and eigenvectors are crucial because they directly control **how data is compressed, which features are emphasized, and how much information is preserved in PCA**.\n"
      ],
      "metadata": {
        "id": "QAftosvy02Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
        " - **KNN and PCA complement each other very effectively when used together in a single machine learning pipeline because PCA improves the quality and efficiency of KNN’s distance-based predictions.** Since KNN relies completely on distance calculations between data points, its performance is highly affected by the **curse of dimensionality**, where high-dimensional data makes all points appear similarly distant, leading to poor accuracy and high computation time. PCA helps solve this problem by **reducing the number of features while retaining the most important variance in the data**, which removes noise, redundancy, and less useful information. When PCA is applied before KNN, the dataset becomes **lower-dimensional, cleaner, and more structured**, making distance calculations more meaningful and reliable. This results in **faster predictions, lower memory usage, and often higher accuracy** for KNN. In simple terms, **PCA prepares and simplifies the data**, while **KNN performs better classification or regression on this optimized feature space**, making them a powerful combination for real-world datasets with many features such as images, medical data, and sensor data.\n"
      ],
      "metadata": {
        "id": "gKVnjOpr0-D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.  Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        " - The KNN classifier performs much better after feature scaling because KNN relies on distance calculations. Without scaling, features with large values dominate the distance computation and reduce accuracy. After applying StandardScaler, all features are brought to the same scale, making distance calculations fair and improving the model’s accuracy significantly. This proves that feature scaling is very important for KNN."
      ],
      "metadata": {
        "id": "il1L-9UJ1GuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine Dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ----------- KNN WITHOUT FEATURE SCALING -----------\n",
        "knn_without_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_without_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_without = knn_without_scaling.predict(X_test)\n",
        "accuracy_without = accuracy_score(y_test, y_pred_without)\n",
        "\n",
        "# ----------- KNN WITH FEATURE SCALING -----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_with_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_with = knn_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with = accuracy_score(y_test, y_pred_with)\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy without Feature Scaling:\", accuracy_without)\n",
        "print(\"Accuracy with Feature Scaling:\", accuracy_with)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV4zAlgb1TfC",
        "outputId": "3e36ea8c-fa82-4e9b-aabd-cff79e0c902f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Feature Scaling: 0.7222222222222222\n",
            "Accuracy with Feature Scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        " - The explained variance ratio shows how much information (variance) each principal component captures from the original dataset. The first few components capture most of the important information, which is why PCA is effective for dimensionality reduction."
      ],
      "metadata": {
        "id": "lGy520Vj1btr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Feature Scaling (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print Explained Variance Ratio\n",
        "print(\"Explained Variance Ratio of Each Principal Component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {var:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgOV1FDy1mZJ",
        "outputId": "98664197-5352-4be9-f560-b015454a9590"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of Each Principal Component:\n",
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        " - The KNN classifier gives higher accuracy on the original scaled dataset because it uses all 13 features of the Wine dataset. When PCA is applied and only the top 2 principal components are retained, some information is lost during dimensionality reduction, which leads to a slight drop in accuracy. However, the PCA-based KNN model is much faster, uses less memory, and avoids the curse of dimensionality, making it useful when efficiency is more important than maximum accuracy."
      ],
      "metadata": {
        "id": "4ajgOvp-1sX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine Dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# ------------------ ORIGINAL DATA (WITH SCALING) ------------------\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train, y_train)\n",
        "\n",
        "y_pred_original = knn_original.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ------------------ PCA WITH TOP 2 COMPONENTS ------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
        "    X_pca, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train_pca)\n",
        "\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
        "\n",
        "# ------------------ RESULTS ------------------\n",
        "print(\"Accuracy on Original Dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA (2 Components) Dataset:\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgEMBVf310y6",
        "outputId": "3da499a3-edd9-422b-fb50-78746aad1845"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Dataset: 0.9444444444444444\n",
            "Accuracy on PCA (2 Components) Dataset: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        " - From the results, we observe that the Euclidean distance metric gives slightly higher accuracy than the Manhattan distance metric on the scaled Wine dataset. Since the dataset is properly standardized, Euclidean distance effectively measures the true geometric distance between points. Manhattan distance also performs well, but it is more sensitive to feature-wise variations. This experiment shows that the choice of distance metric directly affects KNN performance, and Euclidean distance is generally preferred for well-scaled continuous datasets like the Wine dataset."
      ],
      "metadata": {
        "id": "swheEnn516Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine Dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------- KNN with Euclidean Distance --------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -------- KNN with Manhattan Distance --------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy using Euclidean Distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy using Manhattan Distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sqKqSXz2G1C",
        "outputId": "c2d7872a-6cbf-4b4f-ae17-3fa9f6e9ecde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Euclidean Distance: 0.9444444444444444\n",
            "Accuracy using Manhattan Distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples,traditional models overfit.\n",
        " - For a high-dimensional, low-sample-size gene expression problem I would first standardize features (PCA and KNN are distance/signal sensitive), then apply PCA to remove noise and redundant dimensions while preserving most variance — this reduces the curse of dimensionality and the overfitting risk. I would choose the number of components using a mix of (a) an explained-variance threshold (e.g., 90–99%) to see how many PCs capture most variance, and (b) cross-validation (GridSearchCV over candidate n_components) to select the dimensionality that gives the best downstream performance. Next I would train KNN on the PCA-transformed data (tuning k and optionally the distance metric) inside a pipeline so scaling→PCA→KNN is evaluated together. For evaluation I’d use stratified cross-validation (or nested CV for final model selection), plus hold-out test performance, and report accuracy along with class-sensitive metrics (precision, recall, F1) and the confusion matrix; I’d also check stability across folds and learning curves to ensure generalization. To justify this pipeline to stakeholders: emphasize that PCA reduces noise and dimensionality (so fewer false patterns), KNN is simple and interpretable in the reduced space, cross-validation controls overfitting, and final hold-out validation demonstrates real expected performance — all important in biomedical settings where reproducibility and robustness matter. Below is a practical Python script that implements this pipeline (scalable to real gene data) and example output you can expect."
      ],
      "metadata": {
        "id": "IuM-utdY2Jrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full reproducible example pipeline for a high-dimensional gene-expression-like dataset.\n",
        "# - Simulates a high-dim dataset (replace simulation with your real gene-expression matrix X,y)\n",
        "# - StandardScaler -> PCA -> KNN in Pipeline\n",
        "# - GridSearchCV tunes n_components and n_neighbors (nested CV recommended for production)\n",
        "# - Final evaluation on hold-out test set with classification report & confusion matrix\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------- Replace this with your real data ----------\n",
        "# For demo: simulate 100 samples and 1000 features (50 informative)\n",
        "X, y = make_classification(n_samples=100, n_features=1000, n_informative=50,\n",
        "                           n_redundant=50, n_classes=3, random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Train / Hold-out split ----------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ---------- Inspect explained variance to guide choices ----------\n",
        "scaler_for_pca = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler_for_pca.transform(X_train)\n",
        "pca_full = PCA().fit(X_train_scaled)\n",
        "explained_ratio = pca_full.explained_variance_ratio_\n",
        "cumulative = np.cumsum(explained_ratio)\n",
        "# Example: how many PCs to reach 95% variance\n",
        "n_comp_95 = int(np.searchsorted(cumulative, 0.95) + 1)\n",
        "print(\"Components to reach 95% variance (approx):\", n_comp_95)\n",
        "\n",
        "# ---------- Pipeline + GridSearch to jointly pick PCA components and K ----------\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA()),           # n_components will be tuned\n",
        "    (\"knn\", KNeighborsClassifier())  # n_neighbors will be tuned\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"pca__n_components\": [2, 5, 10, 20, n_comp_95],\n",
        "    \"knn__n_neighbors\": [3, 5, 7],\n",
        "    \"knn__metric\": ['euclidean']   # optionally include 'manhattan'\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params from CV:\", grid.best_params_)\n",
        "print(\"Best CV accuracy on training folds:\", grid.best_score_)\n",
        "\n",
        "# ---------- Final evaluation on hold-out test set ----------\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Hold-out test accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification report on test set:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ---------- Quick summary ----------\n",
        "print(\"Selected PCA components:\", best_model.named_steps['pca'].n_components_)\n",
        "print(\"Selected K for KNN:\", best_model.named_steps['knn'].n_neighbors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07TmgUif2deV",
        "outputId": "42480a92-a3f3-4750-c1db-8bc7abbf3e0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Components to reach 95% variance (approx): 72\n",
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 75.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 654, in fit\n",
            "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 588, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/memory.py\", line 326, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\", line 468, in fit_transform\n",
            "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
            "                                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\", line 542, in _fit\n",
            "    return self._fit_full(X, n_components, xp, is_array_api_compliant)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/decomposition/_pca.py\", line 556, in _fit_full\n",
            "    raise ValueError(\n",
            "ValueError: n_components=72 must be between 0 and min(n_samples, n_features)=64 with svd_solver='full'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.3    0.325  0.3375 0.3125    nan 0.325  0.3875 0.325  0.275     nan\n",
            " 0.3625 0.2875 0.3125 0.3125    nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params from CV: {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'pca__n_components': 5}\n",
            "Best CV accuracy on training folds: 0.3875\n",
            "Hold-out test accuracy: 0.2\n",
            "Classification report on test set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2500    0.4286    0.3158         7\n",
            "           1     0.1429    0.1429    0.1429         7\n",
            "           2     0.0000    0.0000    0.0000         6\n",
            "\n",
            "    accuracy                         0.2000        20\n",
            "   macro avg     0.1310    0.1905    0.1529        20\n",
            "weighted avg     0.1375    0.2000    0.1605        20\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[3 4 0]\n",
            " [5 1 1]\n",
            " [4 2 0]]\n",
            "Selected PCA components: 5\n",
            "Selected K for KNN: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why PCA first: reduces noise, removes redundant features, mitigates curse of dimensionality, speeds up training and inference.\n",
        "\n",
        "How many components: use explained-variance threshold (e.g., 90–99%) as a first guide, then tune with cross-validation to pick the number that produces the best predictive performance downstream (tradeoff between information retained and model complexity).\n",
        "\n",
        "Why KNN after PCA: KNN is simple and benefits when distances are meaningful; PCA makes distances more meaningful by removing irrelevant dimensions.\n",
        "\n",
        "Evaluation: use stratified cross-validation, report class-balanced metrics (precision/recall/F1), confusion matrix, and test on a held-out set; for high-stakes biomedical tasks, perform nested CV, repeated CV, and external validation (independent cohort) where possible.\n",
        "\n",
        "Robustness & reproducibility: tune hyperparameters and report variance across folds; document pipeline steps, random seeds, and preprocessing so results can be reproduced.\n",
        "\n",
        "Biological interpretability: while PCA components are linear combinations (less interpretable than original genes), you can inspect loadings of top PCs to find which genes contribute most and follow up with domain analysis (pathways, prior biological knowledge)."
      ],
      "metadata": {
        "id": "h7bqgfbv2rv5"
      }
    }
  ]
}